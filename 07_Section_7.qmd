---
title: XGBoost
format: html
navbar: false
filters:
  - pyodide
---

## XGBoost

XGBoost (Extreme Gradient Boosting) is an advanced implementation of gradient boosting for supervised learning. It is designed to be highly efficient, flexible, and portable, enhancing performance by using more accurate approximations to find the best tree model. XGBoost builds multiple decision trees sequentially, where each tree tries to correct the errors of the previous one, improving accuracy by focusing on hard-to-predict instances.

![](assets/img/xgboost.png)

### Key Differences from Random Forest:

- Gradient Boosting: XGBoost uses gradient boosting, which builds trees sequentially to correct errors from previous trees, while Random Forest builds trees independently and combines their results.
- Regularization: XGBoost includes regularization parameters to control overfitting, providing more precise control over the model complexity.
- Handling Missing Data: XGBoost has a built-in method for handling missing data during training.

## Implementation of Algorithm

Next, we will run XGBoost on our training dataset, then compute the performance of the metrics using our dataset.


```{pyodide-python}
## importing libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, roc_auc_score, confusion_matrix

# reading data and defining features
data_url = "https://raw.githubusercontent.com/zadchin/econ-module/master/cleaned_loandata.csv"
data = pd.read_csv(data_url)
features = data.drop("Risk_Flag", axis = 1) 
target = data.Risk_Flag 

# train-test split
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.3, random_state=42)

# print to make sure train-test split is completed
print("X train shape: ", X_train.shape)
print("X test shape: ", X_test.shape)

# Call on XGBoost function imported
XGB_model = XGBClassifier(n_estimators=200, max_depth=None, scale_pos_weight=1)

# Train the model on X_train, y_train (training data)
XGB_model = XGB_model.fit(X_train, y_train)

# Test the model on X_test (testing data)
y_pred_XGB = XGB_model.predict(X_test)

# Print result
print("Accuracy: ", np.round(accuracy_score(y_test, y_pred_XGB), 2))
print("Precision: ", np.round(precision_score(y_test, y_pred_XGB), 2))
print("Recall: ", np.round(recall_score(y_test, y_pred_XGB), 2))
print("F1 Score: ", np.round(f1_score(y_test, y_pred_XGB), 2))
```


## Discussion: What are the pros and cons of XGBoost?

$\,$
