---
title: "Baseline"
format: html
filters:
  - pyodide
---

## Building Baseline

A baseline model serves as a simple reference point for evaluating the performance of more complex models. It represents the minimum level of performance that any model should achieve. Common baselines include predicting the most frequent class or random guessing.


**Establishing a baseline is crucial because it provides a benchmark to compare against.** It helps in understanding whether the added complexity of a model is actually improving performance. If a model does not perform better than the baseline, it indicates that the model might not be useful.

The train-test split code to build the baseline models. **Do not change the following code**. Just run it! :)

```{pyodide-python}
## importing libraries
import pandas as pd
from sklearn.model_selection import train_test_split

## calling function from library to read data
data_url = "https://raw.githubusercontent.com/zadchin/econ-module/master/cleaned_loandata.csv"
data = pd.read_csv(data_url)

## constant assigning to determine features & target
features = data.drop("Risk_Flag", axis = 1) 
target = data.Risk_Flag 

## calling function from library to train-test split 
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.3, random_state=42)
print("Train Test Split Completed")
```


## Understanding the metrics

We are going to investigate 4 metrics to evalaute the performance of our machine learning algorithms, namely, accuracy, precision, recall and F1 score. We will go through it one-by-one in the following:

### Accuracy

The ratio of correctly predicted instances to the total instances. Mathematically it is defined as $$\text{Accuracy} =\frac{TP+TN}{\text{Total}}$$


::: {.callout-note collapse="true" appearance="simple"}

## Why can't we blindly trust accuracy?

Will be released after the section

<!-- In our dataset, 30% of the people are diagnosed with pancreatic cancer, and 70% are not. If we rely only on accuracy, our model could just predict everyone as not having cancer and still be 70% accurate. This doesn't help us identify those who actually have cancer. That's why we need to look at other metrics like precision and recall to ensure our model is correctly identifying both those with and without the disease.-->

:::

### Precision

Before we move on to talk about precision, recall & F1 score, we would like you to think about the following problem:

::: {.callout-note collapse="true" appearance="simple"}

## What is the impact of false positive and false negative in defaulting loans?

Will be released after the section

:::


Thus, **precision, recall and F1 score are important metrics to help us to get a hollistic picture of our model performance.**


Precision is defined as the ratio of true positive predictions to the total predicted positives. Mathematically,
$$\text{Precision} =\frac{TP}{TP+FP}$$
 
 
### Recall

Recall is defined as the ratio of true positive predictions to the total actual positives. Mathematically, it is defined as 

$$\text{Recall} = \frac{TP}{TP+FN}$$


### F1 Score

F1 Score is the harmonic mean of precision and recall. Mathematically, it is defined as 

$$\text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$

The F1 Score provides a balance between precision and recall. It is particularly useful when the class distribution is imbalanced, as it considers both false positives and false negatives.

## Baseline 1: All False

First, we will create a baseline around the most frequent class, which in our case, is 0, where we assume all customers are not going to default loan.This baseline mimics a current system where we assume that all customers are not defaulting loans.


```{pyodide-python}
# importing libraries
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import numpy as np

# defining result 
y_pred_baseline = [0] * len(y_test)
print("Baseline (All Negative) Model")
print("Accuracy: ", np.round(accuracy_score(y_test, y_pred_baseline), 2))
print("Precision: ", np.round(precision_score(y_test, y_pred_baseline, zero_division=0),2))
print("Recall: ", np.round(recall_score(y_test, y_pred_baseline, zero_division=0),2))
print("F1 Score: ", np.round(f1_score(y_test, y_pred_baseline, zero_division=0),2))
```


::: {.callout-note collapse="true" appearance="simple"}

## Why is precision, recall and F1 score are all 0 in our case?

Will be released after the section

<!-- Precision, recall, and F1 score are all zero because the baseline model predicts all patients as negative (no PDAC), resulting in no true positives! -->

**Did you realize the importance of different metrics in judging performance?**

:::

## Baseline 2: Random Guess

Random guessing is a baseline method where predictions are made by randomly assigning classes (positive or negative) without any consideration of the input features. As an analogy, it is like you flipping a coin and randomly assign a patient of its status based on the coins (for example, default loan if the coin is head, and has not going to default loan if the coin is tail).

Random guessing sets a very low bar for model performance. If a machine learning model cannot outperform random guessing, it indicates that the model has not captured any meaningful patterns from the data.  Beating random guessing demonstrates that the model has predictive power and can provide reliable results. 


```{pyodide-python}
np.random.seed(42)
y_pred_random = np.random.randint(2, size=len(y_test))
y_pred_baseline = [0] * len(y_test)
print("Baseline (Random Guessing) Model")
print("Accuracy: ", np.round(accuracy_score(y_test, y_pred_random), 2))
print("Precision: ", np.round(precision_score(y_test, y_pred_random, zero_division=0), 2))
print("Recall: ", np.round(recall_score(y_test, y_pred_random, zero_division=0), 2))
print("F1 Score: ", np.round(f1_score(y_test, y_pred_random, zero_division=0), 2))
```


$\,$
