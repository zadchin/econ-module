---
title: Random Forest
format: html
navbar: false
filters:
  - pyodide
---

## Random Forest

Random Forests are an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes of the individual trees. Starting from multiple root nodes, each tree in the forest makes decisions based on subsets of features and data. This process helps in reducing overfitting and improving the model's accuracy by averaging the results of different trees. An Visualization of decision tree is shown below:

![](assets/img/randomforest.png)

### Key difference from Decision Trees:

- Ensemble Learning: This is the concept of combining multiple models to produce a better overall result
- Bootstrap Sampling: Each decision tree in the forest is trained on a different subset of the data, chosen randomly with replacement
- Random Feature Selection: When splitting nodes, each tree in the forest considers a random subset of features, rather than all features. This helps create diverse trees.


## Train-Test Split

As we have done previously under logistic regression, we will first perform a train-test split.

```{pyodide-python}
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, roc_auc_score, confusion_matrix

data_url = "https://raw.githubusercontent.com/zadchin/econ-module/master/cleaned_loandata.csv"
data = pd.read_csv(data_url)
features = data.drop("Risk_Flag", axis = 1) 
target = data.Risk_Flag 
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.3, random_state=42)
print("X train shape: ", X_train.shape)
print("X test shape: ", X_test.shape)
```


## Algorithm

Next, we will run a Random Forest classifier on our training dataset, then compute the performance of the metrics using our dataset.

```{pyodide-python}
import numpy as np
RF_model = RandomForestClassifier(n_estimators=200, max_depth=None,
                                  class_weight="balanced")
RF_model = RF_model.fit(X_train, y_train)
y_pred_RF = RF_model.predict(X_test)
print("Accuracy: ", np.round(accuracy_score(y_test, y_pred_RF), 2))
print("Precision: ",np.round(precision_score(y_test, y_pred_RF), 2))
print("Recall: ",np.round(recall_score(y_test, y_pred_RF), 2))
print("F1 Score: ", np.round(f1_score(y_test, y_pred_RF), 2))
```

## Discussion: What are the pros and cons of Random Forest?



$\,$
