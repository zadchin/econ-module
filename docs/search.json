[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predicting Loan Defaults",
    "section": "",
    "text": "Problem Statement\nImagine a scenario where a customer approaches your bank for a loan. How do you decide if this loan should be granted? More importantly, how do you set the interest rate that accurately reflects the risk involved?\nThis decision-making process is where the concept of classification in data science comes into play, especially in the banking sector. By leveraging historical data, including information on past borrowers’ profiles and their loan repayment histories, you can build predictive models. These models assess the probability of default for new loan applicants, guiding banks to make informed and equitable lending decisions.",
    "crumbs": [
      "Problem Statement"
    ]
  },
  {
    "objectID": "index.html#discussion-1",
    "href": "index.html#discussion-1",
    "title": "Predicting Loan Defaults",
    "section": "Discussion (1)",
    "text": "Discussion (1)\nWhy is it important to predict whether customers will default?\nPlease Wait for the Slido to load",
    "crumbs": [
      "Problem Statement"
    ]
  },
  {
    "objectID": "index.html#discussion-2",
    "href": "index.html#discussion-2",
    "title": "Predicting Loan Defaults",
    "section": "Discussion (2)",
    "text": "Discussion (2)\nWhat can we use to predict predict whether customers will default?\n\n\\(\\,\\)",
    "crumbs": [
      "Problem Statement"
    ]
  },
  {
    "objectID": "01_Section_1.html",
    "href": "01_Section_1.html",
    "title": "1  Understanding the Data",
    "section": "",
    "text": "1.1 Overview\nThis dataset is provided as part of a hackathon. It includes a variety of variables collected at the time of the loan application, intended to facilitate the development and testing of predictive models focusing on loan defaults.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Understanding the Data</span>"
    ]
  },
  {
    "objectID": "01_Section_1.html#a-look-at-the-dataset",
    "href": "01_Section_1.html#a-look-at-the-dataset",
    "title": "1  Understanding the Data",
    "section": "1.2 A look at the Dataset",
    "text": "1.2 A look at the Dataset\nThe dataset contains 18778 rows and 13 columns. Here are the 100 example rows:\n\n\n\n\n\n\n\n\n\n\n\n\nWe defined features and target variable as follow:\n\nFeatures: These are the input variables or predictors in a dataset that are used to make predictions or understand patterns.\nTarget Variable: This is the output variable or the outcome you want to predict or explain, based on the features.\n\n\n\n\n\n1.2.1 Columns\nThe dataset comprises several attributes related to the applicants’ personal and professional backgrounds, captured at the point of their loan application. Below is a detailed description of each column in the dataset:\n\nincome: The income of the user (Type: int)\nage: The age of the user (Type: int)\nexperience: The professional experience of the user in years (Type: int)\nprofession: The profession of the user (Type: string)\nmarried: Indicates whether the user is married or single (Type: string)\nhouse_ownership: Specifies whether the house is owned, rented, or neither (Type: string)\ncar_ownership: Indicates whether the user owns a car (Type: string)\nrisk_flag: Shows whether the user has defaulted on a loan in the past (Type: string)\ncurrent_job_years: The number of years the user has been in their current job (Type: int)\ncurrent_house_years: The number of years the user has been living in their current residence (Type: int)\ncity: The city of residence of the user (Type: string)\nstate: The state of residence of the user (Type: string)\nrisk_flag: A categorical variable indicating whether the applicant has previously defaulted on a loa\n\n\\(\\,\\)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Understanding the Data</span>"
    ]
  },
  {
    "objectID": "02_Section_2.html",
    "href": "02_Section_2.html",
    "title": "2  Visualizing the Data",
    "section": "",
    "text": "2.1 Exploratory Data Analysis\nExploratory Data Analysis (EDA) is the process of analyzing data sets to summarize their main characteristics, often using visual methods.\nIt’s a critical step in the data analysis workflow, providing insights into the structure, patterns, and relationships within the data.\nIn short, EDA is important for the following reason:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing the Data</span>"
    ]
  },
  {
    "objectID": "02_Section_2.html#exploratory-data-analysis",
    "href": "02_Section_2.html#exploratory-data-analysis",
    "title": "2  Visualizing the Data",
    "section": "",
    "text": "EDA helps you to understand the nuances in your data\nEDA helps ensure the quality and reliability of your data\nEDA is the foundation for complex analyses as it helps with features selection, hypothesis generation, and determining the research direction.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing the Data</span>"
    ]
  },
  {
    "objectID": "02_Section_2.html#categorial-features",
    "href": "02_Section_2.html#categorial-features",
    "title": "2  Visualizing the Data",
    "section": "2.2 Categorial Features",
    "text": "2.2 Categorial Features\nFeatures that can only take in a specific set of values, often counted in whole number. For example,\n\nCoin flips: 0 or 1\nDays of the Week: Monday, Tuesday, Wednesday etc\nClass Grades: Students can receive specific grades like A, B, C, D, or F.\n\nTo visualize categorical variables, we normally use bar plot.\n\n2.2.1 Bar Plot\nA bar plot is a chart that uses bars to represent the frequency or count of different categories of a discrete variable.\nBar plots are ideal for discrete variables because they clearly show the count or frequency of each category, making it easy to compare them.\n\n\n\n\n\n\nSelect a categorical column to visualize:\n\ndata = FileAttachment(\"loandata_sampled.csv\").csv({ typed: true })\ncat_columns = ['Married/Single', 'House_Ownership', 'Car_Ownership','Risk_Flag', 'CURRENT_HOUSE_YRS']\nviewof selected_column_cat = Inputs.radio(cat_columns, {value: cat_columns[0]})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngrouped_data = d3.groups(data, d =&gt; d[selected_column_cat]).map(([key, values]) =&gt; ({key, count: values.length}));\n\nPlot.plot({\n  marks: [\n    Plot.barY(grouped_data, {x: \"key\", y: \"count\", fill: \"steelblue\"}),\n    Plot.text(grouped_data, {x: \"key\", y: \"count\", text: d =&gt; d.count, dy: -10})\n  ],\n  x: {\n    label: selected_column_cat\n  },\n  y: {\n    label: \"Count\"\n  },\n  color: {\n    legend: false\n  }\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.2 What did you observe?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing the Data</span>"
    ]
  },
  {
    "objectID": "02_Section_2.html#continuous-features",
    "href": "02_Section_2.html#continuous-features",
    "title": "2  Visualizing the Data",
    "section": "2.3 Continuous Features",
    "text": "2.3 Continuous Features\nContinuous variables are features that can take any value within a given range, often measured and including fractions or decimals. For example,\n\nHeight: A person’s height can be any value within a range (e.g., 150.5 cm, 170.2 cm).\nTemperature: Temperature can vary continuously (e.g., 22.3°C, 35.6°C).\nWeight: Weight can take on any value within a range (e.g., 55.5 kg, 72.8 kg).\n\nWe uses histogram to visualize the continuous variables.\n\n2.3.1 Histogram\nA histogram is a chart that uses bars to represent the distribution of a continuous variable, showing the frequency of data within certain ranges or bins.\nHistogram makes it easier to identify patterns, such as skewness, central tendency, and the presence of outliers.\n\n\n\n\n\n\nSelect a continuous column to visualize:\n\ncont_columns = ['Income', 'Age', 'Experience']\nviewof selected_column = Inputs.radio(cont_columns, {value: 'Income'})\nviewof bin_count = Inputs.range([1, 50], {step: 1, value: 15, label: \"Number of Bins\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  marks: [\n    Plot.rectY(data, Plot.binX({y: \"count\"}, {x: selected_column, fill: \"steelblue\", thresholds: bin_count}))\n  ],\n  x: {\n    label: selected_column\n  },\n  y: {\n    label: \"Count\"\n  },\n  color: {\n    legend: false\n  }\n})",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing the Data</span>"
    ]
  },
  {
    "objectID": "02_Section_2.html#relationship-analysis",
    "href": "02_Section_2.html#relationship-analysis",
    "title": "2  Visualizing the Data",
    "section": "2.4 Relationship Analysis",
    "text": "2.4 Relationship Analysis\n\n2.4.1 Between Categorical Variable\n\n\n\n\n\n\nSelect a column to compare against diagnosis:\n\nbivariate_cat_columns =['Married/Single', 'House_Ownership', 'Car_Ownership']\nviewof selected_bivariate_cat_column = Inputs.radio(bivariate_cat_columns, {value: bivariate_cat_columns[0]})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunction calculateGroupedPercentages(data, groupColumn, stackColumn) {\n  const grouped = d3.rollup(data, v =&gt; v.length, d =&gt; d[groupColumn], d =&gt; d[stackColumn]);\n  const totals = d3.rollup(data, v =&gt; v.length, d =&gt; d[groupColumn]);\n\n  return Array.from(grouped, ([key, values]) =&gt; {\n    const total = totals.get(key);\n    return Array.from(values, ([stackKey, count]) =&gt; ({\n      group: key,\n      stack: stackKey,\n      count,\n      percentage: (count / total) * 100\n    }));\n  }).flat();\n}\n\ngrouped_bivariate_data = calculateGroupedPercentages(data, 'Risk_Flag', selected_bivariate_cat_column);\n\nPlot.plot({\n  marks: [\n    Plot.barY(grouped_bivariate_data, {\n      x: d =&gt; d.group + \":\" + d.stack,\n      y: \"percentage\",\n      fill: \"stack\",\n      title: d =&gt; `${d.stack}: ${d.percentage.toFixed(1)}%`\n    }),\n    Plot.text(grouped_bivariate_data, {\n      x: d =&gt; d.group + \":\" + d.stack,\n      y: d =&gt; d.percentage,\n      text: d =&gt; `${d.percentage.toFixed(1)}%`,\n      dy: -4\n    })\n  ],\n  x: {\n    label: \"Loan Default\",\n    domain: Array.from(new Set(grouped_bivariate_data.map(d =&gt; d.group + \":\" + d.stack))),\n    tickFormat: d =&gt; d.split(\":\")[0] // Format ticks to show only the group\n  },\n  y: {\n    label: \"Percentage\"\n  },\n  color: {\n    legend: true\n  }\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4.2 Between Continuous Variables\n\n2.4.2.1 Correlation\nCorrelation measures how two things are related to each other, like how one changes when the other does.\n\nPositive correlation means as one thing increases, the other also increases (e.g., more study time, better grades).\nNegative correlation means as one thing increases, the other decreases (e.g., more exercise, lower stress).\n\n\n\n\n\n\n\nSelect Continuous Variables for Correlation Heatmap:\n\nbivariate_cont_columns = ['Income', 'Age', 'Experience', 'CURRENT_HOUSE_YRS','Risk_Flag']\nviewof selected_bivariate_cont_columns = Inputs.checkbox(bivariate_cont_columns, {value: bivariate_cont_columns})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunction calculateCorrelationMatrix(data, selectedColumns) {\n  const n = selectedColumns.length;\n  const correlationMatrix = Array.from({ length: n }, () =&gt; Array(n).fill(0));\n\n  function pearsonCorrelation(x, y) {\n    const meanX = d3.mean(x);\n    const meanY = d3.mean(y);\n    const diffX = x.map(d =&gt; d - meanX);\n    const diffY = y.map(d =&gt; d - meanY);\n    const numerator = d3.sum(diffX.map((d, i) =&gt; d * diffY[i]));\n    const denominator = Math.sqrt(d3.sum(diffX.map(d =&gt; d * d)) * d3.sum(diffY.map(d =&gt; d * d)));\n    return numerator / denominator;\n  }\n\n  for (let i = 0; i &lt; n; i++) {\n    for (let j = 0; j &lt; n; j++) {\n      const col1 = selectedColumns[i];\n      const col2 = selectedColumns[j];\n      const values1 = data.map(d =&gt; d[col1]);\n      const values2 = data.map(d =&gt; d[col2]);\n      const correlation = pearsonCorrelation(values1, values2);\n      correlationMatrix[i][j] = correlation;\n    }\n  }\n\n  return correlationMatrix;\n}\n\ncorrelation_matrix = calculateCorrelationMatrix(data, selected_bivariate_cont_columns);\ncorrelation_data = selected_bivariate_cont_columns.flatMap((col1, i) =&gt;\n  selected_bivariate_cont_columns.map((col2, j) =&gt; ({\n    x: col1,\n    y: col2,\n    value: correlation_matrix[i][j]\n  }))\n);\n\nPlot.plot({\n  marks: [\n    Plot.cell(correlation_data, {x: \"x\", y: \"y\", fill: \"value\", title: d =&gt; d.value.toFixed(2)}),\n    Plot.text(correlation_data, {x: \"x\", y: \"y\", text: d =&gt; d.value.toFixed(2), dy: 0, textAnchor: \"middle\"})\n  ],\n  x: {\n    domain: selected_bivariate_cont_columns,\n    label: \"Variables\"\n  },\n  y: {\n    domain: selected_bivariate_cont_columns,\n    label: \"Variables\"\n  },\n  color: {\n    type: \"linear\",\n    scheme: \"blues\",\n    label: \"Correlation\"\n  },\n  width: 600,\n  height: 600\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\,\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing the Data</span>"
    ]
  },
  {
    "objectID": "02_5_Section_2.html",
    "href": "02_5_Section_2.html",
    "title": "3  Train-Test Split",
    "section": "",
    "text": "3.1 Coding in AI: Quick Breakdown\nMoving forward, we will have this framework in mind.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Train-Test Split</span>"
    ]
  },
  {
    "objectID": "02_5_Section_2.html#coding-in-ai-quick-breakdown",
    "href": "02_5_Section_2.html#coding-in-ai-quick-breakdown",
    "title": "3  Train-Test Split",
    "section": "",
    "text": "Calling Libraries: Importing libraries is like visiting a library to borrow tools (functions) you need, such as import numpy as np for numerical operations or import tensorflow as tf for deep learning.\nFunction Implementation: This is where you define the specific tasks your code will perform, like training a model or processing data. Functions are reusable blocks of code that perform specific tasks.\nOther Essentials:\n\nConstant Assignment: Setting values that don’t change during execution, like learning rates or thresholds (learning_rate = 0.01).\nPrinting/Debugging: Using print statements to check the output of your code at various stages, ensuring it behaves as expected (print(model.summary())).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Train-Test Split</span>"
    ]
  },
  {
    "objectID": "02_5_Section_2.html#brief-outline-of-ai-machine-learning",
    "href": "02_5_Section_2.html#brief-outline-of-ai-machine-learning",
    "title": "3  Train-Test Split",
    "section": "3.2 Brief Outline of AI / Machine Learning",
    "text": "3.2 Brief Outline of AI / Machine Learning\n\nGetting Data\nAnalyzing Data, in which we did just now.\nTrain-Test Split\nRunning Baseline\nRunning Model\nImprove and Iterate",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Train-Test Split</span>"
    ]
  },
  {
    "objectID": "02_5_Section_2.html#train-test-split",
    "href": "02_5_Section_2.html#train-test-split",
    "title": "3  Train-Test Split",
    "section": "3.3 Train-Test Split",
    "text": "3.3 Train-Test Split\nThe train-test split is a technique used in AI to divide a dataset into two parts: one for training the model (learning) and one for testing it (evaluating its performance).\n\n\n\n\n\n\nWhy do you think it’s important to test a model on data it hasn’t seen before, rather than just using the training data to evaluate its performance?\n\n\n\n\n\nWill be released after the section\n\n\n\nHere is the code for train-test split:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\\(\\,\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Train-Test Split</span>"
    ]
  },
  {
    "objectID": "03_Section_3.html",
    "href": "03_Section_3.html",
    "title": "4  Baseline",
    "section": "",
    "text": "4.1 Building Baseline\nA baseline model serves as a simple reference point for evaluating the performance of more complex models. It represents the minimum level of performance that any model should achieve. Common baselines include predicting the most frequent class or random guessing.\nEstablishing a baseline is crucial because it provides a benchmark to compare against. It helps in understanding whether the added complexity of a model is actually improving performance. If a model does not perform better than the baseline, it indicates that the model might not be useful.\nThis is train-test split code to build the baseline models. Do not change the following code. Just run it! :)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Baseline</span>"
    ]
  },
  {
    "objectID": "03_Section_3.html#building-baseline",
    "href": "03_Section_3.html#building-baseline",
    "title": "4  Baseline",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Baseline</span>"
    ]
  },
  {
    "objectID": "03_Section_3.html#understanding-the-metrics",
    "href": "03_Section_3.html#understanding-the-metrics",
    "title": "4  Baseline",
    "section": "4.2 Understanding the metrics",
    "text": "4.2 Understanding the metrics\nWe are going to investigate 4 metrics to evalaute the performance of our machine learning algorithms, namely, accuracy, precision, recall and F1 score. We will go through it one-by-one in the following:\n\n4.2.1 Accuracy\nThe ratio of correctly predicted instances to the total instances. Mathematically it is defined as \\[\\text{Accuracy} =\\frac{TP+TN}{\\text{Total}}\\]\n\n\n\n\n\n\nWhy can’t we blindly trust accuracy?\n\n\n\n\n\nWill be released after the section\n\n\n\n\n\n\n4.2.2 Precision\nBefore we move on to talk about precision, recall & F1 score, we would like you to think about the following problem:\n\n\n\n\n\n\nWhat is the impact of false positive and false negative in defaulting loans?\n\n\n\n\n\nWill be released after the section\n\n\n\nThus, precision, recall and F1 score are important metrics to help us to get a hollistic picture of our model performance.\nPrecision is defined as the ratio of true positive predictions to the total predicted positives. Mathematically, \\[\\text{Precision} =\\frac{TP}{TP+FP}\\]\nPrecision tells us how many of the patients predicted to have PDAC actually have the disease. High precision means fewer false positives.\n\n\n4.2.3 Recall\nRecall is defined as the ratio of true positive predictions to the total actual positives. Mathematically, it is defined as\n\\[\\text{Recall} = \\frac{TP}{TP+FN}\\]\nRecall (or sensitivity) indicates how many patients with PDAC are correctly identified by the model. High recall means fewer false negatives, which is crucial for medical diagnosis.\n\n\n4.2.4 F1 Score\nF1 Score is the harmonic mean of precision and recall. Mathematically, it is defined as\n\\[\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\]\nThe F1 Score provides a balance between precision and recall. It is particularly useful when the class distribution is imbalanced, as it considers both false positives and false negatives.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Baseline</span>"
    ]
  },
  {
    "objectID": "03_Section_3.html#baseline-1-all-false",
    "href": "03_Section_3.html#baseline-1-all-false",
    "title": "4  Baseline",
    "section": "4.3 Baseline 1: All False",
    "text": "4.3 Baseline 1: All False\nFirst, we will create a baseline around the most frequent class, which in our case, is 0, where we assume all customers are not going to default loan.This baseline mimics a current system where we assume that all customers are not defaulting loans.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nWhy is precision, recall and F1 score are all 0 in our case?\n\n\n\n\n\nWill be released after the section\n\nDid you realize the importance of different metrics in judging performance?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Baseline</span>"
    ]
  },
  {
    "objectID": "03_Section_3.html#baseline-2-random-guess",
    "href": "03_Section_3.html#baseline-2-random-guess",
    "title": "4  Baseline",
    "section": "4.4 Baseline 2: Random Guess",
    "text": "4.4 Baseline 2: Random Guess\nRandom guessing is a baseline method where predictions are made by randomly assigning classes (positive or negative) without any consideration of the input features. As an analogy, it is like you flipping a coin and randomly assign a patient of its status based on the coins (for example, default loan if the coin is head, and has not going to default loan if the coin is tail).\nRandom guessing sets a very low bar for model performance. If a machine learning model cannot outperform random guessing, it indicates that the model has not captured any meaningful patterns from the data. Beating random guessing demonstrates that the model has predictive power and can provide reliable results.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\\(\\,\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Baseline</span>"
    ]
  },
  {
    "objectID": "04_Section_4.html",
    "href": "04_Section_4.html",
    "title": "5  Logistic Regression",
    "section": "",
    "text": "5.1 Logistic Regression\nLogistic regression is a statistical method used for predicting a binary outcome based on one or more independent variables. The outcome is modeled as a probability between 0 and 1. It models the probability of an event occurring as a function of the independent variables using the sigmoid curve, which is defined as\n\\[\\sigma(x) = \\frac{1}{1+e^{-x}}\\] A visualization of logistic regression compared to linear regression is as follow:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04_Section_4.html#logistic-regression",
    "href": "04_Section_4.html#logistic-regression",
    "title": "5  Logistic Regression",
    "section": "",
    "text": "5.1.1 Importance of logistic regression\n\nInterpretability: The coefficients can be interpreted as the change in log-odds of the outcome for a one-unit change in the predictor.\nHandles non-linear relationships: The sigmoid curve can model non-linear relationships between variables.\nRelatively simple and Provides probability estimates: It’s a simple model and thus less prone to overfitting compared to more complex models. Furthermore, unlike some other classification methods, it gives probabilities of outcomes.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04_Section_4.html#implementation-of-algorithm",
    "href": "04_Section_4.html#implementation-of-algorithm",
    "title": "5  Logistic Regression",
    "section": "5.2 Implementation of Algorithm",
    "text": "5.2 Implementation of Algorithm\nNext, we will run logistic regression on our training dataset, then compute the performance of the metrics using our dataset and then\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04_Section_4.html#discussion-what-is-the-pro-and-cons-of-logistic-regression",
    "href": "04_Section_4.html#discussion-what-is-the-pro-and-cons-of-logistic-regression",
    "title": "5  Logistic Regression",
    "section": "5.3 Discussion: What is the pro and cons of logistic regression?",
    "text": "5.3 Discussion: What is the pro and cons of logistic regression?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "04_Section_4.html#feature-importance",
    "href": "04_Section_4.html#feature-importance",
    "title": "5  Logistic Regression",
    "section": "5.4 Feature Importance",
    "text": "5.4 Feature Importance\nFeature importance indicates how much each input variable (feature) contributes to predicting the outcome. In logistic regression, the log-odds of the outcome are modeled as a linear combination of features:\n\\[log(p/(1-p)) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n\\]\nThe magnitude of each coefficient \\(|\\beta_i|\\) represents how much the log-odds change for a one-unit increase in the corresponding feature \\(x_i\\), holding other features constant. Larger \\(|\\beta_i|\\) implies a stronger effect on the outcome probability, hence greater importance.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\\(\\,\\)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "05_Section_5.html",
    "href": "05_Section_5.html",
    "title": "6  Decision Tree",
    "section": "",
    "text": "6.1 Decision Tree\nDecision trees are tree-like models that make decisions based on asking a series of questions about the features of the data. Starting from a root node, the tree splits the data into subsets based on the most significant attribute at each step. This process continues recursively, forming branches and leaves, until a stopping criterion is met. The leaf nodes represent the final decisions or predictions. A simple decision tree classifying heart disease is as follow:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Decision Tree</span>"
    ]
  },
  {
    "objectID": "05_Section_5.html#train-test-split",
    "href": "05_Section_5.html#train-test-split",
    "title": "6  Decision Tree",
    "section": "6.2 Train-Test Split",
    "text": "6.2 Train-Test Split\nAs we have done previosuly under logistic regression, we will first perform a train-test split.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Decision Tree</span>"
    ]
  },
  {
    "objectID": "05_Section_5.html#algorithm",
    "href": "05_Section_5.html#algorithm",
    "title": "6  Decision Tree",
    "section": "6.3 Algorithm",
    "text": "6.3 Algorithm\nNext, we will run decision treeon our training dataset, then compute the performance of the metrics using our dataset and then\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Decision Tree</span>"
    ]
  },
  {
    "objectID": "05_Section_5.html#discussion-what-is-the-pro-and-cons-of-deciscion-tree",
    "href": "05_Section_5.html#discussion-what-is-the-pro-and-cons-of-deciscion-tree",
    "title": "6  Decision Tree",
    "section": "6.4 Discussion: What is the pro and cons of deciscion tree?",
    "text": "6.4 Discussion: What is the pro and cons of deciscion tree?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Decision Tree</span>"
    ]
  },
  {
    "objectID": "05_Section_5.html#visualizing-the-tree",
    "href": "05_Section_5.html#visualizing-the-tree",
    "title": "6  Decision Tree",
    "section": "6.5 Visualizing the tree",
    "text": "6.5 Visualizing the tree\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\\(\\,\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Decision Tree</span>"
    ]
  },
  {
    "objectID": "06_Section_6.html",
    "href": "06_Section_6.html",
    "title": "7  Random Forest",
    "section": "",
    "text": "7.1 Random Forest\nRandom Forests are an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes of the individual trees. Starting from multiple root nodes, each tree in the forest makes decisions based on subsets of features and data. This process helps in reducing overfitting and improving the model’s accuracy by averaging the results of different trees. An Visualization of decision tree is shown below:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "06_Section_6.html#random-forest",
    "href": "06_Section_6.html#random-forest",
    "title": "7  Random Forest",
    "section": "",
    "text": "7.1.1 Key difference from Decision Trees:\n\nEnsemble Learning: This is the concept of combining multiple models to produce a better overall result\nBootstrap Sampling: Each decision tree in the forest is trained on a different subset of the data, chosen randomly with replacement\nRandom Feature Selection: When splitting nodes, each tree in the forest considers a random subset of features, rather than all features. This helps create diverse trees.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "06_Section_6.html#train-test-split",
    "href": "06_Section_6.html#train-test-split",
    "title": "7  Random Forest",
    "section": "7.2 Train-Test Split",
    "text": "7.2 Train-Test Split\nAs we have done previously under logistic regression, we will first perform a train-test split.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "06_Section_6.html#algorithm",
    "href": "06_Section_6.html#algorithm",
    "title": "7  Random Forest",
    "section": "7.3 Algorithm",
    "text": "7.3 Algorithm\nNext, we will run a Random Forest classifier on our training dataset, then compute the performance of the metrics using our dataset.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  },
  {
    "objectID": "06_Section_6.html#discussion-what-are-the-pros-and-cons-of-random-forest",
    "href": "06_Section_6.html#discussion-what-are-the-pros-and-cons-of-random-forest",
    "title": "7  Random Forest",
    "section": "7.4 Discussion: What are the pros and cons of Random Forest?",
    "text": "7.4 Discussion: What are the pros and cons of Random Forest?\n\\(\\,\\)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  }
]